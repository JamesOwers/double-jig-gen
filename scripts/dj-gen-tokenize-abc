#!/usr/bin/env python
"""Script to tokenise abc data."""
import logging
import os
from functools import partial
from pathlib import Path
from typing import Optional

import fire
import pandas as pd
from tqdm import tqdm as std_tqdm

from double_jig_gen.data import (
    TOKEN_SEPARATOR,
    clean_and_standardise_token,
    fix_encoding_errors,
    remove_quoted_strings,
)
from double_jig_gen.tokenizers import ABCTune, ABCTuneError

logging.basicConfig()
LOGGER = logging.getLogger(__name__)
# https://github.com/tqdm/tqdm/issues/370
tqdm = partial(std_tqdm, dynamic_ncols=True)


def clean_tune_str(tune_str):
    return fix_encoding_errors(remove_quoted_strings(tune_str))


def get_abc_tune(abc_data):
    try:
        abc_tune = ABCTune(
            abc_data,
            # pianoroll_divisions_per_quarternote=12,
            # min_pitch=0,
            # min_time=0,
            # transpose_to_pitchclass="C",
        )
    except ABCTuneError as e:
        msg = (
            f"Not including the following tune:\n{abc_data}\n"
            f"It raised an error when parsing with ABCTune(): {e}."
        )
        LOGGER.warning(msg)
        abc_tune = msg
    except Exception as e:
        msg = (
            f"Not including the following tune:\n{abc_data}\n"
            f"It raised an unhandled error: {e}."
        )
        LOGGER.warning(msg)
        abc_tune = msg
    return abc_tune


DATA_PATH = "data/raw/folk-rnn/data_v1"
OUTPUT_PATH = "data/working/folk-rnn/clean-folk-rnn.txt"


def main(
    data_path: str = DATA_PATH,
    output_path: str = OUTPUT_PATH,
    token_separator: str = TOKEN_SEPARATOR,
    nr_tunes: Optional[int] = None,
    # TODO: reduce this number to zero and clean duff tokens manually (it's currently )
    # partially used to get count = 1 tokens which are just crap (i.e. music21 parse
    # errors) but there are many legitimate count = 1 tokens e.g. chords, accents, etc.
    minimum_token_frequency: Optional[int] = 2,
    log_level: Optional[str] = None,
):
    if log_level is not None:
        LOGGER.setLevel(log_level)
    LOGGER.info(
        f"Tokenizing data from {data_path} and writing to {output_path} using token "
        f"separator {token_separator}"
    )
    output_dir = Path(output_path).parent
    if not output_dir.exists():
        LOGGER.warning(f"Creating output directory {output_dir}")
        output_dir.mkdir(parents=True, exist_ok=False)
    with open(data_path, "r") as fh:
        raw_folkrnn_data = fh.read()
    abc_data_list = [tune_str.strip() for tune_str in raw_folkrnn_data.split("\n\n")]
    orig_nr_tunes = len(abc_data_list)
    LOGGER.info(f"Read {orig_nr_tunes} tunes from {data_path}")

    if nr_tunes is not None:
        LOGGER.info(f"Restricting read to first {nr_tunes} tunes...")
        abc_data_list = abc_data_list[:nr_tunes]
    else:
        nr_tunes = len(abc_data_list)

    token_separator_in_tune = [
        token_separator in "".join(tune) for tune in abc_data_list
    ]
    if any(token_separator_in_tune):
        LOGGER.error(
            f"Can't use token separator {token_separator} because it is contained "
            f"within {sum(token_separator_in_tune)} tunes."
        )
        tunes_with_token_separator = [
            tune for ii, tune in enumerate(abc_data_list) if token_separator_in_tune[ii]
        ]
        tunes_with_token_separator = "\n\n".join(tunes_with_token_separator)
        raise ValueError(
            f"The token separator {token_separator} is contained within the following "
            f"tunes:\n\n{tunes_with_token_separator}"
        )

    LOGGER.info("Checking for duplicate tunes")
    seen_tunes = set()
    abc_data_with_source_idx = []
    for source_idx, tune in enumerate(abc_data_list):
        if tune not in seen_tunes:
            abc_data_with_source_idx.append((source_idx, tune))
            seen_tunes.add(tune)
    dedup_nr_tunes = len(abc_data_with_source_idx)
    nr_duplicate_tunes = nr_tunes - dedup_nr_tunes
    LOGGER.info(
        f"Removed {nr_duplicate_tunes} identical tunes, we now have {dedup_nr_tunes}"
    )

    LOGGER.info(
        "Cleaning the text strings representing the tunes e.g. fixing encoding errors "
        "and removing double quoted text comments."
    )
    clean_abc_data = [
        (source_idx, clean_tune_str(tune_str))
        for source_idx, tune_str in tqdm(
            abc_data_with_source_idx, desc="cleaning tunes"
        )
    ]

    # TODO: profile this
    LOGGER.info("Splitting into tokens using music21 (drop tunes which cant be parsed)")
    tunes = [
        (source_idx, get_abc_tune(abc_data))
        for source_idx, abc_data in tqdm(clean_abc_data, desc="parsing with music21")
    ]
    clean_tunes = [
        (source_idx, tune) for source_idx, tune in tunes if not isinstance(tune, str)
    ]

    LOGGER.info("Cleaning individual tokens")
    tunes_as_token_lists = [
        (
            source_idx,
            [clean_and_standardise_token(tok.src) for tok in tune._abc_handler.tokens],
        )
        for source_idx, tune in tqdm(clean_tunes, desc="splitting into tokens")
    ]

    LOGGER.info(
        "Dropping tunes with rare tokens i.e. tokens which only appear a few times in "
        "the whole corpus"
    )
    nr_tunes_before = len(tunes_as_token_lists)
    token_counts = pd.Series(
        [
            tok
            for tune in tunes  # Note that we're only tokenizing the training data
            for tok in tune
        ]
    ).value_counts()
    is_removed = token_counts < minimum_token_frequency
    removed_tokens = token_counts[is_removed].index
    nr_removed_tokens = sum(is_removed)
    tunes_as_token_lists = [
        (source_idx, tune)
        for source_idx, tune in tqdm(
            tunes_as_token_lists, desc="removing tunes with rare tokens"
        )
        if not any(token in removed_tokens for token in tune)
    ]
    nr_removed_tunes = nr_tunes_before - len(tunes_as_token_lists)
    LOGGER.info(
        f"Removed {nr_removed_tokens} tokens and the {nr_removed_tunes} tunes which "
        "used them."
    )

    LOGGER.info(f"Writing tokenized data to {output_path}")
    source_idx, tunes_as_token_strings = zip(
        *[
            (str(source_idx), token_separator.join(tune))
            for source_idx, tune in tunes_as_token_lists
        ]
    )
    with open(output_path, "w") as fh:
        fh.write("\n".join(tunes_as_token_strings))
    source_idx_path = f"{os.path.splitext(output_path)[0]}_source_idx.txt"
    LOGGER.info(
        "Writing indices of source tunes (i.e. tune index number for tunes in "
        f"{data_path}) for tokenized tunes {output_path} to {source_idx_path}"
    )
    with open(source_idx_path, "w") as fh:
        fh.write("\n".join(source_idx))


if __name__ == "__main__":
    fire.Fire(main)
