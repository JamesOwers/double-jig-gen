{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import music21\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "from double_jig_gen.data import ABCDataset, get_oneills_dataloaders, get_folkrnn_dataloaders\n",
    "from double_jig_gen.tokenizers import Tokenizer, ABCTune, ABCTuneError\n",
    "\n",
    "logging.basicConfig()\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "LOGGER.setLevel(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE_ID = 7\n",
    "# DATA_HOME = \"/disk/scratch_fast/s0816700/data\"\n",
    "DATA_HOME = \"data\"\n",
    "DATA_PATH = f\"{DATA_HOME}/folk-rnn/data_v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â https://github.com/IraKorshunova/folk-rnn/blob/master/configurations/config5.py\n",
    "ONE_HOT = True\n",
    "EMBEDDING_SIZE = 256  # is ignored if one_hot=True\n",
    "NUM_LAYERS = 3\n",
    "RNN_SIZE = 512\n",
    "DROPOUT = 0.5\n",
    "\n",
    "LEARNING_RATE = 0.003\n",
    "LEARNING_RATE_DECAY_AFTER = 20\n",
    "LEARNING_RATE_DECAY = 0.97\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "MAX_EPOCH = 100\n",
    "GRAD_CLIPPING = 5\n",
    "VALIDATION_FRACTION = 0.05\n",
    "VALIDATE_EVERY = 1000  # iterations\n",
    "\n",
    "SAVE_EVERY = 10  # epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGER.info(f\"Changing to device {DEVICE_ID}\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{DEVICE_ID}\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LOGGER.info(f\"device = {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH, 'r') as fh:\n",
    "    raw_folkrnn_data = fh.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_folkrnn_data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_data = raw_folkrnn_data.split(\"\\n\\n\")[0]\n",
    "abc_tune = ABCTune(\n",
    "    abc_data,\n",
    "    pianoroll_divisions_per_quarternote=2,\n",
    "    min_pitch=None,\n",
    "    min_time=None,\n",
    "    transpose_to_pitchclass=\"C\",\n",
    ")\n",
    "abc_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_tune.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import asyncio\n",
    "\n",
    "# def background(f):\n",
    "#     def wrapped(*args, **kwargs):\n",
    "#         return asyncio.get_event_loop().run_in_executor(None, f, *args, **kwargs)\n",
    "#     return wrapped\n",
    "# @background\n",
    "\n",
    "def get_abc_tune(abc_data):\n",
    "#     with warnings.catch_warnings():\n",
    "#         warnings.filterwarnings(\"error\")\n",
    "    try:\n",
    "        abc_tune = ABCTune(\n",
    "            abc_data,\n",
    "            pianoroll_divisions_per_quarternote=12,\n",
    "            min_pitch=0,\n",
    "            min_time=0,\n",
    "            transpose_to_pitchclass=\"C\",\n",
    "        )\n",
    "#         tokens = [str(e) for e in abc_tune.abc_music21.flat]\n",
    "#     except Warning as w:\n",
    "#         print(f\"Not including {abc_data}.\\nRaised warning {w}\")\n",
    "    except ABCTuneError as e:\n",
    "        msg = f\"Raised error: {repr(e)}\\nNot including {abc_data}.\"\n",
    "        LOGGER.warning(msg)\n",
    "        abc_tune = msg\n",
    "    except Exception as e:\n",
    "        msg = f\"unexpected error: {repr(e)}\\nNot including {abc_data}.\"\n",
    "        LOGGER.warning(msg)\n",
    "        abc_tune = msg\n",
    "    return abc_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm._instances.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_data_list = raw_folkrnn_data.split(\"\\n\\n\")\n",
    "len(abc_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_tunes = 1_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: extract tokens\n",
    "# TODO: clean tunes with warnings e.g remove all text in quotes before extraction?\n",
    "tunes = [get_abc_tune(abc_data) for abc_data in tqdm(abc_data_list[:nr_tunes])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tunes = [tune for tune in tunes if not isinstance(tune, str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_tunes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune = clean_tunes[0]\n",
    "tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tune.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_tune = ABCTune(\"T: maitune\\nM:3/4\\nL:1/8\\n|: [ACE]2B2D2 | [ceg]6 | [1 (3ABC (3DEF (3GAB :| [2 (3ABC (3DEF (3GAe |]\")\n",
    "[tok for tok in abc_tune.abc_music21.flat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_tune.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_tune.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abc_tune.events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = copy.deepcopy(tune.tokens[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vars(tune._abc_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = copy.deepcopy(tune._abc_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler.tokenProcess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "handler.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "elem_list = [elem for elem in tune.abc_music21.recurse()]\n",
    "elem_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elem_list[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import music21\n",
    "score = music21.stream.Score()\n",
    "part = music21.stream.Part()\n",
    "nr_measures = 4\n",
    "for _ in range(nr_measures):\n",
    "    measure = music21.stream.Measure()\n",
    "    notes = ((\"A\", 1), (\"B-\", 0.5), (\"C#\", 1.5))\n",
    "    for pitch_name, quarter_length in notes:\n",
    "        measure.append(music21.note.Note(pitch_name, quarterLength=quarter_length))\n",
    "    part.append(measure)\n",
    "score.insert(0, part)\n",
    "score.show(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tune.abc_music21.show(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(tok.src for tok in tune._abc_handler.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "handler.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.preParse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.quarterLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tune.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[tok for tok in tune.abc_music21.flat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tune.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tunes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tunes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bum_tunes = [(idx, tune) for idx, tune in enumerate(tunes) if isinstance(tune, str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bum_tunes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\", \".join([str(idx) for idx, msg in bum_tunes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bum_tunes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx, msg in bum_tunes:\n",
    "    print(idx)\n",
    "    print(msg)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bum_tune_abc_reencoded = [\n",
    "    bytes(\n",
    "        abc_data_list[idx], \"utf-8\"\n",
    "    ).replace(\n",
    "        b\"\\xc3\\xa2\\xc2\\x80\\xc2\\x99\",\n",
    "        \"'\".encode(\"utf-8\")\n",
    "    ).decode(\"utf-8\")\n",
    "    for idx, _ in bum_tunes\n",
    "]\n",
    "bum_tune_abc_reencoded[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extra_tunes = [get_abc_tune(abc_data) for abc_data in tqdm(bum_tune_abc_reencoded)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bum_bum_tunes = [(idx, tune) for idx, tune in enumerate(extra_tunes) if isinstance(tune, str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bum_bum_tunes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx, msg in bum_bum_tunes:\n",
    "    print(idx)\n",
    "    print(msg)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunes[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm._instances.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tunes = Parallel(n_jobs=-1)(delayed(get_abc_tune)(abc_data) for abc_data in raw_folkrnn_data.split(\"\\n\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tunes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data are tokens separated by spaces, and pieces separated by to `\\n` characters. Pieces begin with a meter, then a new line with a key, then a new line with the piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_tunes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tunes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_str_tunes = [[tok.src for tok in tune._abc_handler.tokens] for tune in tqdm(clean_tunes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_set = set(tok for tune in tokenized_str_tunes for tok in tune if not tok.startswith(\"T:\"))\n",
    "vocab_size = len(tokens_set)\n",
    "print(f\"vocabulary size: {vocab_size}\")\n",
    "print(f\"vocabulary (each token separated by a space): \\n{' '.join(sorted(tokens_set))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get frequency of each and exclude infrequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(tokens=tokens_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tunes = [tokenizer.tokenize(tune) for tune in tqdm(tokenized_str_tunes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = ABCDataset(filepath=DATA_PATH)\n",
    "dataset = ABCDataset(\n",
    "    tunes=[\" \".join(token_list) for token_list in tokenized_str_tunes],\n",
    "    tokens=tokens_set,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exclude long tunes\n",
    "We will need to batch all the data. Exclude very long examples for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_lens = np.array([len(t) for t in tokenized_str_tunes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(131)\n",
    "sns.histplot(tune_lens)\n",
    "plt.subplot(132)\n",
    "sns.kdeplot(tune_lens)\n",
    "plt.subplot(133)\n",
    "sns.ecdfplot(tune_lens)\n",
    "plt.suptitle(f\"Number of tokens for all {len(tokenized_str_tunes)} tunes\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct = .99\n",
    "nr_kept = int(np.rint(len(tune_lens)*pct))\n",
    "val_pct = sorted(tune_lens)[nr_kept - 1]\n",
    "val_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_tunes = [tune for tune in tokenized_str_tunes if len(tune) <= val_pct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_tune_lens = np.array([len(t) for t in short_tunes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(131)\n",
    "sns.histplot(short_tune_lens)\n",
    "plt.subplot(132)\n",
    "sns.kdeplot(short_tune_lens)\n",
    "plt.subplot(133)\n",
    "sns.ecdfplot(short_tune_lens)\n",
    "plt.suptitle(f\"Tunes shorter than or equal to {val_pct} tokens\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntunes = len(short_tunes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvalid_tunes = ntunes * VALIDATION_FRACTION\n",
    "# round to a multiple of batch_size\n",
    "nvalid_tunes = BATCH_SIZE * max(\n",
    "    1,\n",
    "    int(np.rint(nvalid_tunes / BATCH_SIZE))\n",
    ")\n",
    "nvalid_tunes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "valid_idxs = rng.choice(np.arange(ntunes), nvalid_tunes, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain_tunes = ntunes - nvalid_tunes\n",
    "train_idxs = np.delete(np.arange(ntunes), valid_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_tunes = [tune for idx, tune in enumerate(short_tunes) if idx in valid_idxs]\n",
    "train_tunes = [tune for idx, tune in enumerate(short_tunes) if idx in train_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = ABCDataset(tunes=valid_tunes, tokens=dataset.tokens)\n",
    "train_dataset = ABCDataset(tunes=train_tunes, tokens=dataset.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.tokenizer.untokenize(train_dataset[0])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.tokenizer.untokenize(valid_dataset[0])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching in the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "PAD_IDX = dataset.tokenizer.pad_token_index\n",
    "\n",
    "def rpad_batch(batch):\n",
    "    # Subtract 1 from the sequence length - never want to predict on </s> token\n",
    "    lengths = [seq.shape[0] for seq in batch]\n",
    "    data = pad_sequence(batch, batch_first=False, padding_value=PAD_IDX)\n",
    "    return data, lengths\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=rpad_batch,\n",
    "    pin_memory=True,\n",
    "#     num_workers=8,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    print(batch[0].size())\n",
    "    print(max(batch[1]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=rpad_batch,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "from double_jig_gen.models import SimpleRNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightning_trainer = pl.Trainer(gpus='1,')\n",
    "lightning_trainer = pl.Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleRNN(\n",
    "    rnn_type=\"LSTM\",\n",
    "    ntoken=dataset.vocabulary_size,\n",
    "    ninp=EMBEDDING_SIZE,\n",
    "    nhid=RNN_SIZE,\n",
    "    nlayers=NUM_LAYERS,\n",
    "    model_batch_size=BATCH_SIZE,\n",
    "    dropout=DROPOUT,\n",
    "    embedding_padding_idx=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lightning_trainer.fit(\n",
    "    model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = dict(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict['encoder_layer.weight'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show that something has been learned! The first four tokens are:\n",
    "* 0: `<pad>` - padding token\n",
    "* 1: `<unk>` - unknown/rare token\n",
    "* 2: `<s>` - start sequence\n",
    "* 3: `</s>` - end sequence\n",
    "\n",
    "The encoder weights show that: nothing is learned for `<pad>`, `<unk>`, and `</s>` as they have their initialised weights near zero; something is learned for `<s>` as these have weights. This is as expected because nothing should follow pad and end seq, and there are no unk tokens in this dataset!\n",
    "\n",
    "The decoder weights show the same, except nothing is learned for `<s>`, and something for `</s>`. Again, this is expected since the start sequence token should never be predicted, and the end sequence token should be predicted a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_enc = param_dict['encoder_layer.weight'].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(W_enc)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(W_enc[:4], aspect='auto', interpolation='none')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_dec = param_dict['decoder_layer.weight'].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(W_dec)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(W_dec[:4], aspect='auto', interpolation='none')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = val_dataloader.dataset.tokenizer\n",
    "# token_sequences = [\n",
    "#     [\"<s>\"],\n",
    "#     [\"<s>\", \"M:6/8\"],\n",
    "#     [\"<s>\", \"M:6/8\", \"K:mix\"],\n",
    "#     [\"\"]\n",
    "# ]\n",
    "token_sequences = (\n",
    "    [[\"blarg\"]] * 1 +\n",
    "    [[\"<s>\"]] * 1 +\n",
    "    [[\"<s>\", \"T: My title\"]] * 1 +\n",
    "    [[\"<s>\", \"T: My title\", \"M: 6/8\"]] * 1 +\n",
    "    [[\"<s>\", \"T: My title\", \"M: 6/8\", \"L: 1/8\"]] * 1 +\n",
    "    [[\"<s>\", \"T: My title\", \"M: 6/8\", \"L: 1/8\", \"K: Cmaj\"]] * 1\n",
    ")\n",
    "priming_dataset = ABCDataset(\n",
    "    tunes=token_sequences,\n",
    "    tokens=val_dataloader.dataset.tokens,\n",
    "    wrap_tunes=False,\n",
    ")\n",
    "pad_token_idx = val_dataloader.dataset.tokenizer.pad_token_index\n",
    "pad_priming_batch = lambda batch: pad_batch(batch, pad_token_idx)\n",
    "priming_loader = DataLoader(\n",
    "    priming_dataset,\n",
    "    batch_size=len(priming_dataset),\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # TODO: fix why we can't use workers...\n",
    "    pin_memory=False,\n",
    "    collate_fn=pad_priming_batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_item in tqdm(priming_loader):\n",
    "    print(batch_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from double_jig_gen.data import pad_batch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "end_token_idx = tokenizer.end_token_index\n",
    "max_seq_len = 1000\n",
    "for batch_item in tqdm(priming_loader):\n",
    "    padded_data, seq_lens = batch_item\n",
    "    seq_lens = np.array(seq_lens)\n",
    "    padded_data = padded_data.to(device)\n",
    "    still_generating = np.array([True] * padded_data.shape[1])\n",
    "    for ii in tqdm(range(max_seq_len), leave=False):\n",
    "        next_tokens = model.generate_next_token(\n",
    "            padded_data[:, still_generating], \n",
    "            seq_lens[still_generating],\n",
    "            topk=5\n",
    "        )\n",
    "        padded_data = F.pad(\n",
    "            input=padded_data,\n",
    "            pad=(0, 0, 0, 1),  # Pad bottom\n",
    "            mode=\"constant\",\n",
    "            value=pad_token_idx,\n",
    "        )\n",
    "        padded_data[seq_lens[still_generating], still_generating] = next_tokens\n",
    "        if all(padded_data[-1] == 0):\n",
    "            padded_data = padded_data[:-1]\n",
    "        seq_lens[still_generating] += 1\n",
    "        last_tokens = padded_data[seq_lens - 1, range(padded_data.shape[1])]\n",
    "        still_generating = np.array((last_tokens != end_token_idx).tolist())\n",
    "        if still_generating.sum() == 0:\n",
    "            break\n",
    "        \n",
    "    generations = [tokenizer.untokenize(seq.cpu()) for seq in padded_data.T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "#     \" \".join(gen[:10]) + \" ... \" + \" \".join(gen[(gen.index(\"</s>\")-10):gen.index(\"</s>\") + 1]) \n",
    "    \" \".join(gen[:10]) + \" ... \" + \" \".join(gen[(gen_len-10):gen_len]) \n",
    "    for gen, gen_len in zip(generations, seq_lens)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gen(gen_list):\n",
    "    start_token, title, meter, note_len, key, *tune, end_token = gen_list\n",
    "    return f\"T: {title.strip()}\\n{meter.strip()}\\n{note_len.strip()}\\n{key.strip()}\\n{' '.join(tune)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, gen in enumerate(generations):\n",
    "    tune_str = clean_gen(gen[:seq_lens[idx]])\n",
    "    print(tune_str)\n",
    "    tune = ABCTune(tune_str)\n",
    "    tune.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on oneills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_dataset = ABCDataset(\n",
    "    filepath='data/oneills_reformat.abc',\n",
    "    tokens=dataset.tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(on_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_unk_toks = 0\n",
    "for idx in range(len(on_dataset)):\n",
    "    nr_unk_toks += (\n",
    "        np.array(on_dataset[idx]) == on_dataset.tokenizer.unk_token_index\n",
    "    ).sum()\n",
    "nr_unk_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_dataloader = DataLoader(\n",
    "    on_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=rpad_batch,\n",
    "    pin_memory=True,\n",
    "    num_workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = pl.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.00,\n",
    "    patience=100,\n",
    "    verbose=True,\n",
    "    mode=\"min\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_trainer = pl.Trainer(gpus='0,', deterministic=True, early_stop_callback=early_stop_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_trainer.fit(\n",
    "    model,\n",
    "    train_dataloader=on_dataloader,\n",
    "    val_dataloaders=on_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn, vld, tst = get_oneills_dataloaders(\n",
    "    \"/disk/scratch_fast/s0816700/data/oneills/oneills_reformat.abc\",\n",
    "    \"/disk/scratch_fast/s0816700/data/folk-rnn/data_v3_vocabulary.txt\",\n",
    "    batch_size=16,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(len(tst.dataset)):\n",
    "    print(tst.dataset[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "lightning_trainer.test(\n",
    "    model,\n",
    "    test_dataloaders=tst,\n",
    "#             ckpt_path=str(args.model_load_from_checkpoint),\n",
    "    ckpt_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
